---
layout: post
title: Getting Started with Scrapy
---

# Getting Started with Scrapy

This post is a quick overview of getting started with _[Scrapy](https://scrapy.org/)_, an open source Python-based web scraping tool that is easy to install from the shell:

```shell
pip install Scrapy
```

or, via Anaconda:

```shell
conda install -c conda-forge scrapy
```
### Scrapy Shell

_Scrapy_ provides a handy shell for testing out bits of code before starting your project. Simply type ``scrapy shell`` from your preferred shell. We'll start by using the shell to run a crawler and look at the response, then attempt to find pertinent DOM elements to scrape. To demonstrate the usefulness of Scrapy, we're going to scrape some world markets data from [Google Finance](https://finance.google.com/finance):

![World Markets](https://github.com/strongdan/blog/blob/gh-pages/assets/world_markets.png)

Typing `scrapy shell 'https://finance.google.com/finance' --nolog` will fetch the URL and return ojects it retrieves:

![Scrapy Response](https://github.com/strongdan/blog/blob/gh-pages/assets/scrapy_response.png)

_Scrapy_ provides a number of useful shortcut methods: `fetch()`, `shelp()`, and `view(response)`. At this point, _Scrapy_ has already made the request, so you don't need to use the `fetch()` method (which fetches a new response from the given request and updates all related objects) again. The Google Finance response is stored in the response object, which can be viewed by typing `print(response.text)`. This will return the entire webpage HTML, which is pretty messy, so we'll need to go and view the original page using Chrome Dev Tools (or equivalent). Typing `view(response)` will open up the webpage in your default browser and you can use developer tools to locate the World Markets div within the DOM. The table we need has a class of `quotes`, which we can use to retrieve the data with _Scrapy's_ `.css()` method. 





### Starting a New Project

This project follows the _Scrapy_ [documentation](https://docs.scrapy.org/en/latest/) tutorial for setting up and running a _Scrapy_ project, writing a spider to crawl a site, and exporting the scraped data. Starting a new project is as simple as typing ``scrapy startproject [project name]`` in the shell. For the tutorial, we'll type ``scrapy startproject tutorial``. This will create a basic tutorial directory structure:

```python
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/

https://bigishdata.com/2017/05/11/general-tips-for-web-scraping-with-python/

### Some Things to Keep in Mind
[Jack Schultz](https://twitter.com/jack_schultz) has some incredibly useful [guidelines](https://bigishdata.com/2017/05/11/general-tips-for-web-scraping-with-python/) for smart, ethical and friendly web scraping on his [Big-ish Data Blog](https://bigishdata.com/):
* Check if the site has an API First
* Figure out the URLs of all the data
* Check JSON loading of data
* Fall back to HTML scraping
* CSV is probably a fine type of database initially (Database is useful if you have data that keeps coming)
* Scrape with header that has your name and email so company knows it’s you
* Make sure you don’t keep hitting the servers
* He also suggests using the [gevent](http://www.gevent.org/) Python library, but I'll save that for a future post

Make sure you take the time to read his post carefully and be thoughful as you venture into this brave new world of web scraping.

### Links
[Scrapy Documentation](https://docs.scrapy.org/en/latest/)
